{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import random\n",
    "from scipy import stats\n",
    "import glob\n",
    "import math\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "import seaborn as sns\n",
    "plt.rcParams['figure.figsize'] = (20.0, 10.0)\n",
    "plt.rcParams['font.family'] = \"serif\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's begin putting together the transcriptome simulation set\n",
    "\n",
    "# What we need\n",
    "# number of transcripts to simulate (what is an average number of transcripts per tissue?)\n",
    "# number of loci (average for a tissue)\n",
    "# for each locus need to draw the number of transcripts of each category, such that the final distributions\n",
    "#     are close to tissue averages\n",
    "\n",
    "# then for each transcript need to draw from the distribution of total contribution to expression\n",
    "#     such that the final distributions resemble those observed in real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declarations\n",
    "base_dir_data = \"/home/sparrow/JHU/gtex_stats/data/\"\n",
    "base_dir_out = \"/home/sparrow/JHU/gtex_stats/out_test/\"\n",
    "out_dir = \"/home/sparrow/JHU/tx_noise/sim_samples_single_joint_rsem/\"\n",
    "hg38_fa = \"/home/sparrow/genomicData/hg38/hg38_p8.fa\"\n",
    "\n",
    "genRNAseq = \"/home/varabyou/genomicTools/genRNAseq_noFold_cov.R\"\n",
    "\n",
    "readlen = 101\n",
    "num_samples = 2\n",
    "\n",
    "gff3cols=[\"seqid\",\"source\",\"type\",\"start\",\"end\",\"score\",\"strand\",\"phase\",\"attributes\"]\n",
    "rsem_tpm_cols = [\"transcript_id\",\"gene_id\",\"length\",\"effective_length\",\"expected_count\",\"TPM\",\"FPKM\",\"IsoPct\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {\n",
    "        \"starting number of real and splicing noise loci\":0,\n",
    "        \"starting number of splicing noise loci\":0,\n",
    "        \"starting number of intronic noise loci\":0,\n",
    "        \"starting number of polymerase noise loci\": 0,\n",
    "        \"adjusted starting number of real only loci\":0,\n",
    "        \"adjusted starting number of splicing noise loci\":0,\n",
    "        \"adjusted starting number of intronic noise loci\":0,\n",
    "        \"adjusted starting number of polymerase noise loci\": 0,\n",
    "        \"average number of real loci\":0,\n",
    "        \"average number of noise loci\":0,\n",
    "        \"average number of undefined loci\":0,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>loading base annotations\n"
     ]
    }
   ],
   "source": [
    "# load base annotations\n",
    "print(\">>>loading base annotations\")\n",
    "real_baseDF = pd.read_csv(base_dir_data+\"ALL.combined.IDs.olny.in.ALL.combined.true.gtf\",sep=\"\\t\",names=gff3cols)\n",
    "real_baseDF = real_baseDF[real_baseDF[\"type\"]==\"transcript\"].reset_index(drop=True)\n",
    "nonint_baseDF = pd.read_csv(base_dir_data+\"ALL.combined.IDs.olny.in.ALL.combined.no.contained.non.intronic.gtf\",sep=\"\\t\",names=gff3cols)\n",
    "nonint_baseDF = nonint_baseDF[nonint_baseDF[\"type\"]==\"transcript\"].reset_index(drop=True)\n",
    "int_baseDF = pd.read_csv(base_dir_data+\"ALL.combined.IDs.olny.in.ALL.combined.no.contained.intronic.gtf\",sep=\"\\t\",names=gff3cols)\n",
    "int_baseDF = int_baseDF[int_baseDF[\"type\"]==\"transcript\"].reset_index(drop=True)\n",
    "pol_baseDF = pd.read_csv(base_dir_data+\"ALL.combined.IDs.olny.in.ALL.combined.no.contained.RNApol.gtf\",sep=\"\\t\",names=gff3cols)\n",
    "pol_baseDF = pol_baseDF[pol_baseDF[\"type\"]==\"transcript\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>getting loci IDs\n"
     ]
    }
   ],
   "source": [
    "# get all loci and transcript IDs\n",
    "print(\">>>getting loci IDs\")\n",
    "real_baseDF[\"lid\"] = real_baseDF[\"attributes\"].str.split(\"gene_id \\\"\",expand=True)[1].str.split(\"\\\"\",expand=True)[0]\n",
    "nonint_baseDF[\"lid\"] = nonint_baseDF[\"attributes\"].str.split(\"gene_id \\\"\",expand=True)[1].str.split(\"\\\"\",expand=True)[0]\n",
    "int_baseDF[\"lid\"] = int_baseDF[\"attributes\"].str.split(\"gene_id \\\"\",expand=True)[1].str.split(\"\\\"\",expand=True)[0]\n",
    "pol_baseDF[\"lid\"] = pol_baseDF[\"attributes\"].str.split(\"gene_id \\\"\",expand=True)[1].str.split(\"\\\"\",expand=True)[0]\n",
    "real_locs = set(real_baseDF[\"lid\"])\n",
    "nonint_locs = set(nonint_baseDF[\"lid\"])\n",
    "int_locs = set(int_baseDF[\"lid\"])\n",
    "pol_locs = set(pol_baseDF[\"lid\"])\n",
    "stats[\"starting number of real and splicing noise loci\"] = len(real_locs)\n",
    "stats[\"starting number of splicing noise loci\"] = len(nonint_locs)\n",
    "stats[\"starting number of intronic loci\"] = len(int_locs)\n",
    "stats[\"starting number of polymerase loci\"] = len(pol_locs)\n",
    "\n",
    "total_n_locs = len(real_locs)+len(nonint_locs)+len(int_locs)+len(pol_locs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform cleanup, by removing any loci in \n",
    "#   1. int that are not in real\n",
    "#   2. nonint that are not in real\n",
    "#   3. pol that are in real\n",
    "int_locs = int_locs - int_locs.difference(real_locs)\n",
    "assert(len(int_locs.difference(real_locs))==0),\"something wrong intronic\"\n",
    "int_baseDF = int_baseDF[int_baseDF[\"lid\"].isin(int_locs)].reset_index(drop=True)\n",
    "\n",
    "nonint_locs = nonint_locs - nonint_locs.difference(real_locs)\n",
    "assert(len(nonint_locs) == len(real_locs.intersection(nonint_locs))),\"something wrong non-intronic\"\n",
    "nonint_baseDF = nonint_baseDF[nonint_baseDF[\"lid\"].isin(nonint_locs)].reset_index(drop=True)\n",
    "\n",
    "pol_locs = pol_locs - real_locs.intersection(pol_locs)\n",
    "assert(len(real_locs.intersection(pol_locs))==0),\"something wrong polymerase\"\n",
    "pol_baseDF = pol_baseDF[pol_baseDF[\"lid\"].isin(pol_locs)].reset_index(drop=True)\n",
    "\n",
    "real_baseDF[\"tid\"] = real_baseDF[\"attributes\"].str.split(\"transcript_id \\\"\",expand=True)[1].str.split(\"\\\"\",expand=True)[0]\n",
    "real_baseDF = real_baseDF[[\"lid\",\"tid\"]]\n",
    "nonint_baseDF[\"tid\"] = nonint_baseDF[\"attributes\"].str.split(\"transcript_id \\\"\",expand=True)[1].str.split(\"\\\"\",expand=True)[0]\n",
    "nonint_baseDF = nonint_baseDF[[\"lid\",\"tid\"]]\n",
    "int_baseDF[\"tid\"] = int_baseDF[\"attributes\"].str.split(\"transcript_id \\\"\",expand=True)[1].str.split(\"\\\"\",expand=True)[0]\n",
    "int_baseDF = int_baseDF[[\"lid\",\"tid\"]]\n",
    "pol_baseDF[\"tid\"] = pol_baseDF[\"attributes\"].str.split(\"transcript_id \\\"\",expand=True)[1].str.split(\"\\\"\",expand=True)[0]\n",
    "pol_baseDF = pol_baseDF[[\"lid\",\"tid\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the averages\n",
    "avg_real_locs = 0\n",
    "avg_noise_locs = 0\n",
    "avg_undefined_locs = 0\n",
    "with open(base_dir_out+\"res_distrib.loc_stats\",\"r\") as inFP:\n",
    "    for line in inFP.readlines():\n",
    "        type_loc,num_loc = line.split(\":\")\n",
    "        if type_loc==\"real\":\n",
    "            avg_real_locs = int(num_loc)\n",
    "        elif type_loc==\"noise\":\n",
    "            avg_noise_locs = int(num_loc)\n",
    "        elif type_loc==\"undefined\":\n",
    "            if(int(num_loc))>0:\n",
    "                avg_undefined_locs = int(num_loc)\n",
    "        else:\n",
    "            print(\"error in loading averages\")\n",
    "stats[\"average number of real loci\"] = avg_real_locs\n",
    "stats[\"average number of noise loci\"] = avg_noise_locs\n",
    "stats[\"average number of undefined loci\"] = avg_undefined_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select loci of each type based on the averages\n",
    "# noise loci are a combination of: intronic and polymerase\n",
    "# this needs to be done proportionally to the consistent parts of each group (real(real,nonint) and noise(int,pol))\n",
    "\n",
    "real_only_locs = real_locs - nonint_locs\n",
    "\n",
    "# in case of inconsistencies associated with merging transcripts of different types under the same locus\n",
    "# we need to scale the averages with respect to the new numbers\n",
    "avg_total_locs = avg_real_locs+avg_noise_locs\n",
    "sum_locs_to_choose_from = len(real_only_locs)+len(nonint_locs)+len(int_locs)+len(pol_locs)\n",
    "\n",
    "stats[\"adjusted number of real only loci\"] = len(real_only_locs)\n",
    "stats[\"adjusted number of splicing noise loci\"] = len(nonint_locs)\n",
    "stats[\"adjusted number of intronic loci\"] = len(int_locs)\n",
    "stats[\"adjusted number of polymerase loci\"] = len(pol_locs)\n",
    "\n",
    "perc_real = len(real_only_locs)/len(real_locs)\n",
    "perc_nonint = len(nonint_locs)/len(real_locs)\n",
    "assert(perc_real+perc_nonint)==1,\"wrong percent real and nonint\"\n",
    "\n",
    "perc_int = len(int_locs)/(len(int_locs)+len(pol_locs))\n",
    "perc_pol = len(pol_locs)/(len(int_locs)+len(pol_locs))\n",
    "assert((perc_int+perc_pol)==1),\"wrong percent pol and int\"\n",
    "stats[\"number of selected real locs\"] = int(avg_real_locs*perc_real)\n",
    "stats[\"number of selected splicing noise locs\"] = int(avg_real_locs*perc_nonint)\n",
    "stats[\"number of selected intronic loci locs\"] = int(avg_noise_locs*perc_int)\n",
    "stats[\"number of selected polymerase locs\"] = int(avg_noise_locs*perc_pol)\n",
    "\n",
    "real_only_locs_rand = np.random.choice(list(real_only_locs),int(avg_real_locs*perc_real), replace=False)\n",
    "nonint_locs_rand = np.random.choice(list(nonint_locs),int(avg_real_locs*perc_nonint), replace=False)\n",
    "int_locs_rand = np.random.choice(list(int_locs),int(avg_noise_locs*perc_int), replace=False)\n",
    "pol_locs_rand = np.random.choice(list(pol_locs),int(avg_noise_locs*perc_pol), replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_only_baseDF = real_baseDF[real_baseDF[\"lid\"].isin(real_only_locs_rand)].reset_index(drop=True)\n",
    "real_nonint_baseDF = real_baseDF[(real_baseDF[\"lid\"].isin(nonint_locs_rand))&~(real_baseDF[\"lid\"].isin(real_only_locs_rand))].reset_index(drop=True)\n",
    "nonint_baseDF = nonint_baseDF[nonint_baseDF[\"lid\"].isin(nonint_locs_rand)].reset_index(drop=True)\n",
    "int_baseDF = int_baseDF[int_baseDF[\"lid\"].isin(int_locs_rand)].reset_index(drop=True)\n",
    "pol_baseDF = pol_baseDF[pol_baseDF[\"lid\"].isin(pol_locs_rand)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the transcript tissue distribution\n",
    "ttx_real = list()\n",
    "ttx_nonint = list()\n",
    "ttx_int = list()\n",
    "ttx_pol = list()\n",
    "\n",
    "tmp_tx = pd.read_csv(base_dir_out+\"res_distrib.num_tx_per_tissue_loc2\")\n",
    "tmp_tx_real_loc = tmp_tx[(tmp_tx[\"real\"]>0)][[\"total\",\"real\",\"nonintronic\"]].reset_index(drop=True)\n",
    "tmp_tx_real_loc[\"perc_real\"] = tmp_tx_real_loc[\"real\"]/tmp_tx_real_loc[\"total\"]\n",
    "tmp_tx_real_loc[\"perc_nonint\"] = tmp_tx_real_loc[\"nonintronic\"]/tmp_tx_real_loc[\"total\"]\n",
    "d_real = list(tmp_tx_real_loc[\"perc_real\"])\n",
    "d_nonint = list(tmp_tx_real_loc[\"perc_nonint\"])\n",
    "\n",
    "tmp_tx_int_loc = tmp_tx[(tmp_tx[\"intronic\"]>0)][[\"total\",\"intronic\"]].reset_index(drop=True)\n",
    "tmp_tx_int_loc[\"perc_int\"] = tmp_tx_int_loc[\"intronic\"]/tmp_tx_int_loc[\"total\"]\n",
    "d_int = list(tmp_tx_int_loc[\"perc_int\"])\n",
    "\n",
    "tmp_tx_pol_loc = tmp_tx[(tmp_tx[\"polymerase\"]>0)][[\"total\",\"polymerase\"]].reset_index(drop=True)\n",
    "tmp_tx_pol_loc[\"perc_pol\"] = tmp_tx_pol_loc[\"polymerase\"]/tmp_tx_pol_loc[\"total\"]\n",
    "d_pol = list(tmp_tx_pol_loc[\"perc_pol\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we have the loci, we can subset the gtfs and move on to working with the transcripts\n",
    "# the question is: how do we allocate transcripts per locus?\n",
    "# do we do this based on the distribution of the number of transcripts per each type of locus?\n",
    "# are there any caveates in the co-dependencies such as between real and non-intronic?\n",
    "\n",
    "# we should probably only select transcripts of each group within each type of locus dataframe\n",
    "\n",
    "# one important consideration is that the total number of transcripts should be close to what is observed\n",
    "#  in an average tissue (needs to be computed in the gtex_stats)\n",
    "# this approach, however, might be overly complicated, and simply randomly selecting n transcripts for each group\n",
    "#  based on distribution computed by gtex_stats might be sufficient, and should approximate well to desired values\n",
    "\n",
    "real_only_txs = set()\n",
    "for name, group in real_only_baseDF.groupby(\"lid\"):\n",
    "    tids = group[\"tid\"].tolist()\n",
    "    nt = math.ceil(d_real[random.randint(0,len(d_real)-1)]*len(tids))\n",
    "    random.shuffle(tids)\n",
    "    real_only_txs.update(tids[:nt])\n",
    "\n",
    "real_nonint_txs = set()\n",
    "for name, group in real_nonint_baseDF.groupby(\"lid\"):\n",
    "    tids = group[\"tid\"].tolist()\n",
    "    nt = math.ceil(d_real[random.randint(0,len(d_real)-1)]*len(tids))\n",
    "    random.shuffle(tids)\n",
    "    real_nonint_txs.update(tids[:nt])\n",
    "\n",
    "nonint_txs = set()\n",
    "for name, group in nonint_baseDF.groupby(\"lid\"):\n",
    "    tids = group[\"tid\"].tolist()\n",
    "    nt = math.ceil(d_nonint[random.randint(0,len(d_nonint)-1)]*len(tids))\n",
    "    random.shuffle(tids)\n",
    "    nonint_txs.update(tids[:nt])\n",
    "\n",
    "int_txs = set()\n",
    "for name, group in int_baseDF.groupby(\"lid\"):\n",
    "    tids = group[\"tid\"].tolist()\n",
    "    nt = math.ceil(d_int[random.randint(0,len(d_int)-1)]*len(tids))\n",
    "    random.shuffle(tids)\n",
    "    int_txs.update(tids[:nt])\n",
    "\n",
    "pol_txs = set()\n",
    "for name, group in pol_baseDF.groupby(\"lid\"):\n",
    "    tids = group[\"tid\"].tolist()\n",
    "    nt = math.ceil(d_pol[random.randint(0,len(d_pol)-1)]*len(tids))\n",
    "    random.shuffle(tids)\n",
    "    pol_txs.update(tids[:nt])\n",
    "\n",
    "stats[\"number of real only txs\"] = len(real_only_txs)\n",
    "stats[\"number of real nonint txs\"] = len(real_nonint_txs)\n",
    "stats[\"number of nonint txs\"] = len(nonint_txs)\n",
    "stats[\"number of int txs\"] = len(int_txs)\n",
    "stats[\"number of pol txs\"] = len(pol_txs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of real only txs before: 11567\n",
      "number of real nonint txs before: 26797\n",
      "number of nonint txs before: 105942\n",
      "number of int txs before: 8676\n",
      "number of pol txs before: 36360\n"
     ]
    }
   ],
   "source": [
    "# now that we have lists of transcripts - we can subset the annotation further\n",
    "print(\"number of real only txs before: \"+str(len(real_only_baseDF)))\n",
    "real_only_baseDF = real_only_baseDF[real_only_baseDF[\"tid\"].isin(real_only_txs)].reset_index(drop=True)\n",
    "print(\"number of real nonint txs before: \"+str(len(real_nonint_baseDF)))\n",
    "real_nonint_baseDF = real_nonint_baseDF[real_nonint_baseDF[\"tid\"].isin(real_nonint_txs)].reset_index(drop=True)\n",
    "print(\"number of nonint txs before: \"+str(len(nonint_baseDF)))\n",
    "nonint_baseDF = nonint_baseDF[nonint_baseDF[\"tid\"].isin(nonint_txs)].reset_index(drop=True)\n",
    "print(\"number of int txs before: \"+str(len(int_baseDF)))\n",
    "int_baseDF = int_baseDF[int_baseDF[\"tid\"].isin(int_txs)].reset_index(drop=True)\n",
    "print(\"number of pol txs before: \"+str(len(pol_baseDF)))\n",
    "pol_baseDF = pol_baseDF[pol_baseDF[\"tid\"].isin(pol_txs)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lastly we need to generate samples based on the underlying annotation\n",
    "# for this we will need additional information, similar to the tissue_per_loc2 in order to tell\n",
    "# how likely a sample is to contribute a transcript to a tissue\n",
    "\n",
    "# load the transcript sample distribution\n",
    "ttx_real = list()\n",
    "ttx_nonint = list()\n",
    "ttx_int = list()\n",
    "ttx_pol = list()\n",
    "\n",
    "tmp_tx = pd.read_csv(base_dir_out+\"res_distrib.num_tx_per_sample_loc2\")\n",
    "tmp_tx_real_loc = tmp_tx[(tmp_tx[\"real\"]>0)][[\"total\",\"real\",\"nonintronic\"]].reset_index(drop=True)\n",
    "tmp_tx_real_loc[\"perc_real\"] = tmp_tx_real_loc[\"real\"]/tmp_tx_real_loc[\"total\"]\n",
    "tmp_tx_real_loc[\"perc_nonint\"] = tmp_tx_real_loc[\"nonintronic\"]/tmp_tx_real_loc[\"total\"]\n",
    "d_real = list(tmp_tx_real_loc[\"perc_real\"])\n",
    "d_nonint = list(tmp_tx_real_loc[\"perc_nonint\"])\n",
    "\n",
    "tmp_tx_int_loc = tmp_tx[(tmp_tx[\"intronic\"]>0)][[\"total\",\"intronic\"]].reset_index(drop=True)\n",
    "tmp_tx_int_loc[\"perc_int\"] = tmp_tx_int_loc[\"intronic\"]/tmp_tx_int_loc[\"total\"]\n",
    "d_int = list(tmp_tx_int_loc[\"perc_int\"])\n",
    "\n",
    "tmp_tx_pol_loc = tmp_tx[(tmp_tx[\"polymerase\"]>0)][[\"total\",\"polymerase\"]].reset_index(drop=True)\n",
    "tmp_tx_pol_loc[\"perc_pol\"] = tmp_tx_pol_loc[\"polymerase\"]/tmp_tx_pol_loc[\"total\"]\n",
    "d_pol = list(tmp_tx_pol_loc[\"perc_pol\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_only_txs = [set() for x in range(num_samples)]\n",
    "for name, group in real_only_baseDF.groupby(\"lid\"):\n",
    "    tids = group[\"tid\"].tolist()\n",
    "    for i in range(num_samples):\n",
    "        nt = math.ceil(d_real[random.randint(0,len(d_real)-1)]*len(tids))\n",
    "        random.shuffle(tids)\n",
    "        real_only_txs[i].update(tids[:nt])\n",
    "\n",
    "real_nonint_txs = [set() for x in range(num_samples)]\n",
    "for name, group in real_nonint_baseDF.groupby(\"lid\"):\n",
    "    tids = group[\"tid\"].tolist()\n",
    "    for i in range(num_samples):\n",
    "        nt = math.ceil(d_real[random.randint(0,len(d_real)-1)]*len(tids))\n",
    "        random.shuffle(tids)\n",
    "        real_nonint_txs[i].update(tids[:nt])\n",
    "\n",
    "nonint_txs = [set() for x in range(num_samples)]\n",
    "for name, group in nonint_baseDF.groupby(\"lid\"):\n",
    "    tids = group[\"tid\"].tolist()\n",
    "    for i in range(num_samples):\n",
    "        nt = math.ceil(d_nonint[random.randint(0,len(d_nonint)-1)]*len(tids))\n",
    "        random.shuffle(tids)\n",
    "        nonint_txs[i].update(tids[:nt])\n",
    "\n",
    "int_txs = [set() for x in range(num_samples)]\n",
    "for name, group in int_baseDF.groupby(\"lid\"):\n",
    "    tids = group[\"tid\"].tolist()\n",
    "    for i in range(num_samples):\n",
    "        nt = math.ceil(d_int[random.randint(0,len(d_int)-1)]*len(tids))\n",
    "        random.shuffle(tids)\n",
    "        int_txs[i].update(tids[:nt])\n",
    "\n",
    "pol_txs = [set() for x in range(num_samples)]\n",
    "for name, group in pol_baseDF.groupby(\"lid\"):\n",
    "    tids = group[\"tid\"].tolist()\n",
    "    for i in range(num_samples):\n",
    "        nt = math.ceil(d_pol[random.randint(0,len(d_pol)-1)]*len(tids))\n",
    "        random.shuffle(tids)\n",
    "        pol_txs[i].update(tids[:nt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the last thing we need to do is to guarantee that no expressions are smaller than 1\n",
    "# the best way to do this is\n",
    "# get the minimum of all coverages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we have sample specific stuff - we can generate sample-specific GTFs\n",
    "int_baseDF = pd.read_csv(base_dir_data+\"ALL.combined.IDs.olny.in.ALL.combined.no.contained.intronic.gtf\",sep=\"\\t\",names=gff3cols)\n",
    "pol_baseDF = pd.read_csv(base_dir_data+\"ALL.combined.IDs.olny.in.ALL.combined.no.contained.RNApol.gtf\",sep=\"\\t\",names=gff3cols)\n",
    "int_baseDF[\"tid\"] = int_baseDF[\"attributes\"].str.split(\"transcript_id \\\"\",expand=True)[1].str.split(\"\\\"\",expand=True)[0]\n",
    "pol_baseDF[\"tid\"] = pol_baseDF[\"attributes\"].str.split(\"transcript_id \\\"\",expand=True)[1].str.split(\"\\\"\",expand=True)[0]\n",
    "\n",
    "for i in range(num_samples):\n",
    "    int_baseDF[int_baseDF[\"tid\"].isin(int_txs[i])][gff3cols].to_csv(out_dir+\"/res_distrib.int.sample\"+str(i)+\".gtf\",sep=\"\\t\",index=False,header=False,quoting=csv.QUOTE_NONE)\n",
    "    pol_baseDF[pol_baseDF[\"tid\"].isin(pol_txs[i])][gff3cols].to_csv(out_dir+\"/res_distrib.pol.sample\"+str(i)+\".gtf\",sep=\"\\t\",index=False,header=False,quoting=csv.QUOTE_NONE)\n",
    "\n",
    "# load distribution of the sum of expressions\n",
    "tmp_exp_int = pd.read_csv(base_dir_out+\"res_distrib.cov_sample_intronic\")\n",
    "d_exp_int = list(tmp_exp_int[\"cov\"]+1)\n",
    "\n",
    "tmp_exp_pol = pd.read_csv(base_dir_out+\"res_distrib.cov_sample_polymerase\")\n",
    "d_exp_pol = list(tmp_exp_pol[\"cov\"]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now generate corresponding expression matrices for the transcripts in each sample for each type\n",
    "for i in range(num_samples):        \n",
    "    intDF = pd.read_csv(out_dir+\"/res_distrib.int.sample\"+str(i)+\".gtf\",sep=\"\\t\",names=gff3cols)\n",
    "    nt_int = len(intDF[intDF[\"type\"]==\"transcript\"])\n",
    "    exps_int = np.random.choice(d_exp_int, nt_int, replace=False)\n",
    "    np.savetxt(out_dir+\"/res_distrib.int.sample\"+str(i)+\".exp\", exps_int, delimiter=',',fmt='%0.2f')\n",
    "\n",
    "    polDF = pd.read_csv(out_dir+\"/res_distrib.pol.sample\"+str(i)+\".gtf\",sep=\"\\t\",names=gff3cols)\n",
    "    nt_pol = len(polDF[polDF[\"type\"]==\"transcript\"])\n",
    "    exps_pol = np.random.choice(d_exp_pol, nt_pol, replace=False)\n",
    "    np.savetxt(out_dir+\"/res_distrib.pol.sample\"+str(i)+\".exp\", exps_pol, delimiter=',',fmt='%0.2f')\n",
    "\n",
    "    # need to generate data for the RSEM simulation\n",
    "    tdf = int_baseDF[int_baseDF[\"tid\"].isin(int_txs[i])]\n",
    "    edf = tdf[tdf[\"type\"]==\"exon\"].reset_index(drop=True)\n",
    "    tdf = tdf[tdf[\"type\"]==\"transcript\"][[\"tid\",\"start\",\"end\"]].reset_index(drop=True) # intended for order\n",
    "    tdf[\"len\"] = tdf[\"end\"]-tdf[\"start\"]\n",
    "    tdf.drop([\"start\",\"end\"],axis=1,inplace=True)\n",
    "    edf[\"elen\"] = edf[\"end\"]-edf[\"start\"]\n",
    "    edf = edf[[\"tid\",\"elen\"]]\n",
    "    edf = edf.groupby(\"tid\").agg({\"elen\":\"sum\"}).reset_index()\n",
    "    # now organize by\n",
    "    tdf = tdf.merge(edf,on=\"tid\",how=\"left\",indicator=True)\n",
    "    assert len(tdf) == len(tdf[tdf[\"_merge\"]==\"both\"]),\"missing/extra exons\"\n",
    "\n",
    "    intDF = pd.read_csv(out_dir+\"/res_distrib.int.sample\"+str(i)+\".gtf\",sep=\"\\t\",names=gff3cols)\n",
    "    nt_int = len(intDF[intDF[\"type\"]==\"transcript\"])\n",
    "    exps_int = np.random.choice(d_exp_int, nt_int, replace=False)\n",
    "    tdf[\"cov\"] = exps_int\n",
    "    tdf[\"cov\"] = tdf[\"cov\"].fillna(0)\n",
    "    assert len(tdf[tdf[\"cov\"]==0])==0,\"missing coverages\"\n",
    "    tdf[\"nr\"] = tdf[\"elen\"]*tdf[\"cov\"]\n",
    "    tdf[\"kelen\"] = tdf[\"elen\"]/1000\n",
    "    tdf[\"rpk\"] = tdf[\"nr\"]/tdf[\"kelen\"]\n",
    "    pm_sf = tdf[\"rpk\"].sum()/1000000\n",
    "    tdf[\"tpm\"] = tdf[\"rpk\"]/pm_sf\n",
    "    tdf.drop([\"_merge\",\"cov\",\"nr\",\"kelen\",\"rpk\"],axis=1,inplace=True)\n",
    "    tdf.columns = [\"transcript_id\",\"length\",\"effective_length\",\"TPM\"]\n",
    "    tdf[\"gene_id\"] = tdf[\"transcript_id\"]\n",
    "    tdf[\"expected_count\"] = 0.00\n",
    "    tdf[\"FPKM\"] = 0.00\n",
    "    tdf[\"IsoPct\"] = 0.00\n",
    "    tdf[rsem_tpm_cols].to_csv(out_dir+\"/res_distrib.int.sample\"+str(i)+\".rsem_tpms\",sep=\"\\t\",index=False)\n",
    "\n",
    "    # need to generate data for the RSEM simulation\n",
    "    tdf = pol_baseDF[pol_baseDF[\"tid\"].isin(pol_txs[i])]\n",
    "    edf = tdf[tdf[\"type\"]==\"exon\"].reset_index(drop=True)\n",
    "    tdf = tdf[tdf[\"type\"]==\"transcript\"][[\"tid\",\"start\",\"end\"]].reset_index(drop=True) # intended for order\n",
    "    tdf[\"len\"] = tdf[\"end\"]-tdf[\"start\"]\n",
    "    tdf.drop([\"start\",\"end\"],axis=1,inplace=True)\n",
    "    edf[\"elen\"] = edf[\"end\"]-edf[\"start\"]\n",
    "    edf = edf[[\"tid\",\"elen\"]]\n",
    "    edf = edf.groupby(\"tid\").agg({\"elen\":\"sum\"}).reset_index()\n",
    "    # now organize by\n",
    "    tdf = tdf.merge(edf,on=\"tid\",how=\"left\",indicator=True)\n",
    "    assert len(tdf) == len(tdf[tdf[\"_merge\"]==\"both\"]),\"missing/extra exons\"\n",
    "\n",
    "    polDF = pd.read_csv(out_dir+\"/res_distrib.pol.sample\"+str(i)+\".gtf\",sep=\"\\t\",names=gff3cols)\n",
    "    nt_pol = len(polDF[polDF[\"type\"]==\"transcript\"])\n",
    "    exps_pol = np.random.choice(d_exp_pol, nt_pol, replace=False)\n",
    "    tdf[\"cov\"] = exps_pol\n",
    "    tdf[\"cov\"] = tdf[\"cov\"].fillna(0)\n",
    "    assert len(tdf[tdf[\"cov\"]==0])==0,\"missing coverages\"\n",
    "    tdf[\"nr\"] = tdf[\"elen\"]*tdf[\"cov\"]\n",
    "    tdf[\"kelen\"] = tdf[\"elen\"]/1000\n",
    "    tdf[\"rpk\"] = tdf[\"nr\"]/tdf[\"kelen\"]\n",
    "    pm_sf = tdf[\"rpk\"].sum()/1000000\n",
    "    tdf[\"tpm\"] = tdf[\"rpk\"]/pm_sf\n",
    "    tdf.drop([\"_merge\",\"cov\",\"nr\",\"kelen\",\"rpk\"],axis=1,inplace=True)\n",
    "    tdf.columns = [\"transcript_id\",\"length\",\"effective_length\",\"TPM\"]\n",
    "    tdf[\"gene_id\"] = tdf[\"transcript_id\"]\n",
    "    tdf[\"expected_count\"] = 0.00\n",
    "    tdf[\"FPKM\"] = 0.00\n",
    "    tdf[\"IsoPct\"] = 0.00\n",
    "    tdf[rsem_tpm_cols].to_csv(out_dir+\"/res_distrib.pol.sample\"+str(i)+\".rsem_tpms\",sep=\"\\t\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now need to somehow get the tpms for the samples\n",
    "# while intronic and polymerase are easy to sample\n",
    "# real and nonintronic are correlated\n",
    "# so we need to use the special distribution to sample\n",
    "\n",
    "# first need to be able to process real and nonintronic together\n",
    "# and output expression accordingly\n",
    "\n",
    "# load joint expressions\n",
    "real_baseDF = pd.read_csv(base_dir_data+\"ALL.combined.IDs.olny.in.ALL.combined.true.gtf\",sep=\"\\t\",names=gff3cols)\n",
    "nonint_baseDF = pd.read_csv(base_dir_data+\"ALL.combined.IDs.olny.in.ALL.combined.no.contained.non.intronic.gtf\",sep=\"\\t\",names=gff3cols)\n",
    "real_baseDF[\"tid\"] = real_baseDF[\"attributes\"].str.split(\"transcript_id \\\"\",expand=True)[1].str.split(\"\\\"\",expand=True)[0]\n",
    "nonint_baseDF[\"tid\"] = nonint_baseDF[\"attributes\"].str.split(\"transcript_id \\\"\",expand=True)[1].str.split(\"\\\"\",expand=True)[0]\n",
    "# load joint expressions\n",
    "joined_df = pd.read_csv(base_dir_out+\"res_distrib.tpms_joined\")\n",
    "joined_df[\"code\"] = joined_df[\"nt_real\"].astype(int).astype(str)+\"-\"+joined_df[\"nt_nonint\"].astype(int).astype(str)\n",
    "joined_df.drop([\"nt_real\",\"nt_nonint\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_samples):\n",
    "    sub_real = real_baseDF[real_baseDF[\"tid\"].isin(real_only_txs[i].union(real_nonint_txs[i]))]\n",
    "    sub_real[\"gid\"] = sub_real[\"attributes\"].str.split(\"gene_id \\\"\",expand=True)[1].str.split(\"\\\"\",expand=True)[0]\n",
    "    sub_nonint = nonint_baseDF[nonint_baseDF[\"tid\"].isin(nonint_txs[i])]\n",
    "    sub_nonint[\"gid\"] = sub_nonint[\"attributes\"].str.split(\"gene_id \\\"\",expand=True)[1].str.split(\"\\\"\",expand=True)[0]\n",
    "\n",
    "    gsr = sub_real[sub_real[\"type\"]==\"transcript\"][[\"gid\",\"tid\"]].groupby(\"gid\").count().reset_index()\n",
    "    gsr.columns = [\"gid\",\"ntr\"]\n",
    "    gsn = sub_nonint[sub_nonint[\"type\"]==\"transcript\"][[\"gid\",\"tid\"]].groupby(\"gid\").count().reset_index()\n",
    "    gsn.columns = [\"gid\",\"ntn\"]\n",
    "    gs = gsr.merge(gsn,on=\"gid\",how=\"outer\",indicator=True)\n",
    "    print(\"removing \"+str(len(gs[gs[\"_merge\"]==\"right_only\"]))+\" number of loci due to presence of nonint only transcripts\")\n",
    "    gs = gs[~(gs[\"_merge\"]==\"right_only\")].reset_index(drop=True)\n",
    "    gs[\"ntn\"] = gs[\"ntn\"].fillna(0)\n",
    "    gs[\"ntr\"] = gs[\"ntr\"].fillna(0)\n",
    "    gs[\"code\"] = gs[\"ntr\"].astype(int).astype(str)+\"-\"+gs[\"ntn\"].astype(int).astype(str)\n",
    "    gs.sort_values(by=\"code\",inplace=True)\n",
    "\n",
    "    real_exp = open(out_dir+\"/res_distrib.real.sample\"+str(i)+\".exp\",\"w+\")\n",
    "    nonint_exp = open(out_dir+\"/res_distrib.nonint.sample\"+str(i)+\".exp\",\"w+\")\n",
    "\n",
    "    jd = joined_df.copy(deep=True)\n",
    "    # compute the number of each code in gs\n",
    "    tmp = gs.groupby(by = 'code')['gid'].count().reset_index()\n",
    "    tmp.columns = [\"code\",\"nc\"]\n",
    "    jd = jd.merge(tmp,on=\"code\",how=\"inner\",indicator=True)\n",
    "    # need to have a way of knowing which loci have no code matches...\n",
    "    # and to maintain the order between the expressions and corresponding gtfs\n",
    "    jd = jd[jd[\"_merge\"]==\"both\"]\n",
    "    gs = gs[gs[\"code\"].isin(set(jd[\"code\"]))]\n",
    "    # now we can select th\n",
    "    jd = jd.groupby('code', as_index=False).apply(lambda obj: obj.loc[np.random.choice(obj.index, obj.nc.iloc[0],replace=True),:]).reset_index(drop=True)\n",
    "\n",
    "    gs.sort_values(by=\"code\",inplace=True,ascending=False)\n",
    "    gs.reset_index(drop=True,inplace=True)\n",
    "    jd.sort_values(by=\"code\",inplace=True,ascending=False)\n",
    "    jd.reset_index(drop=True,inplace=True)\n",
    "    gs[[\"real\",\"nonint\",\"code_jd\"]] = jd[[\"real\",\"nonint\",\"code\"]]\n",
    "    assert len(gs)==len(gs[gs[\"code\"]==gs[\"code_jd\"]]),\"codes do not match\"\n",
    "\n",
    "    # now we can split the expressions\n",
    "    rgs = gs[gs[\"ntr\"]>0][[\"gid\",\"real\"]].reset_index(drop=True)\n",
    "    rgs[\"reals\"] = rgs[\"real\"].str.split(\";\")\n",
    "    rgs = rgs.explode(\"reals\")\n",
    "    rgs.drop(\"real\",axis=1,inplace=True)\n",
    "    # we can save the expressions, however, now we need to make sure the gtf\n",
    "    # is saved in the same order\n",
    "    sub_real = sub_real[sub_real[\"gid\"].isin(rgs[\"gid\"])]\n",
    "    assert len(set(rgs[\"gid\"]).intersection(set(sub_real[\"gid\"])))==len(set(sub_real[\"gid\"])),\"incompatible gids in sub_real and gs\"\n",
    "    # to finally ensure the same order between the two dataframes we can\n",
    "    # get the order of genes in one dataframe\n",
    "    # and then merge into it independently both expression and gtf data\n",
    "    #   to guarantee identical ordering\n",
    "    gene_order = rgs[[\"gid\"]]\n",
    "    gene_order.drop_duplicates(keep=\"first\",inplace=True)\n",
    "    gene_order.reset_index(drop=True)\n",
    "    real_gtf = gene_order.merge(sub_real,on=\"gid\",how=\"inner\")\n",
    "    real_exp = gene_order.merge(rgs,on=\"gid\",how=\"inner\")\n",
    "    real_exp[\"reals\"] = real_exp[\"reals\"].astype(float).round(2)+1.0\n",
    "    real_gtf[gff3cols].to_csv(out_dir+\"/res_distrib.real.sample\"+str(i)+\".gtf\",sep=\"\\t\",index=False,header=False,quoting=csv.QUOTE_NONE)\n",
    "    real_exp[[\"reals\"]].to_csv(out_dir+\"/res_distrib.real.sample\"+str(i)+\".exp\",index=False,header=False)\n",
    "    \n",
    "    # now we can split the expressions for the nonint\n",
    "    ngs = gs[gs[\"ntn\"]>0][[\"gid\",\"nonint\"]].reset_index(drop=True)\n",
    "    ngs[\"nonints\"] = ngs[\"nonint\"].str.split(\";\")\n",
    "    ngs = ngs.explode(\"nonints\")\n",
    "    ngs.drop(\"nonint\",axis=1,inplace=True)\n",
    "    # we can save the expressions, however, now we need to make sure the gtf\n",
    "    # is saved in the same order\n",
    "    sub_nonint = sub_nonint[sub_nonint[\"gid\"].isin(ngs[\"gid\"])]\n",
    "    assert len(set(ngs[\"gid\"]).intersection(set(sub_nonint[\"gid\"])))==len(set(sub_nonint[\"gid\"])),\"incompatible gids in sub_nonint and gs\"\n",
    "    # to finally ensure the same order between the two dataframes we can\n",
    "    # get the order of genes in one dataframe\n",
    "    # and then merge into it independently both expression and gtf data\n",
    "    #   to guarantee identical ordering\n",
    "    gene_order = ngs[[\"gid\"]]\n",
    "    gene_order.drop_duplicates(keep=\"first\",inplace=True)\n",
    "    gene_order.reset_index(drop=True)\n",
    "    nonint_gtf = gene_order.merge(sub_nonint,on=\"gid\",how=\"inner\")\n",
    "    nonint_exp = gene_order.merge(ngs,on=\"gid\",how=\"inner\")\n",
    "    nonint_exp[\"nonints\"] = nonint_exp[\"nonints\"].astype(float).round(2)+1.0\n",
    "    nonint_gtf[gff3cols].to_csv(out_dir+\"/res_distrib.nonint.sample\"+str(i)+\".gtf\",sep=\"\\t\",index=False,header=False,quoting=csv.QUOTE_NONE)\n",
    "    nonint_exp[[\"nonints\"]].to_csv(out_dir+\"/res_distrib.nonint.sample\"+str(i)+\".exp\",index=False,header=False)\n",
    "    \n",
    "    # process for RSEM\n",
    "    edf = real_gtf[real_gtf[\"type\"]==\"exon\"].reset_index(drop=True)\n",
    "    tdf = real_gtf[real_gtf[\"type\"]==\"transcript\"][[\"tid\",\"start\",\"end\"]].reset_index(drop=True) # intended for order\n",
    "    tdf[\"len\"] = tdf[\"end\"]-tdf[\"start\"]\n",
    "    tdf.drop([\"start\",\"end\"],axis=1,inplace=True)\n",
    "    edf[\"elen\"] = edf[\"end\"]-edf[\"start\"]\n",
    "    edf = edf[[\"tid\",\"elen\"]]\n",
    "    edf = edf.groupby(\"tid\").agg({\"elen\":\"sum\"}).reset_index()\n",
    "    # now organize by\n",
    "    tdf = tdf.merge(edf,on=\"tid\",how=\"left\",indicator=True)\n",
    "    assert len(tdf) == len(tdf[tdf[\"_merge\"]==\"both\"]),\"missing/extra exons\"\n",
    "\n",
    "    tdf[\"cov\"] = real_exp[\"reals\"]\n",
    "    tdf[\"cov\"] = tdf[\"cov\"].fillna(0)\n",
    "    assert len(tdf[tdf[\"cov\"]==0])==0,\"missing coverages\"\n",
    "    tdf[\"nr\"] = tdf[\"elen\"]*tdf[\"cov\"]\n",
    "    tdf[\"kelen\"] = tdf[\"elen\"]/1000\n",
    "    tdf[\"rpk\"] = tdf[\"nr\"]/tdf[\"kelen\"]\n",
    "    pm_sf = tdf[\"rpk\"].sum()/1000000\n",
    "    tdf[\"tpm\"] = tdf[\"rpk\"]/pm_sf\n",
    "    tdf.drop([\"_merge\",\"cov\",\"nr\",\"kelen\",\"rpk\"],axis=1,inplace=True)\n",
    "    tdf.columns = [\"transcript_id\",\"length\",\"effective_length\",\"TPM\"]\n",
    "    tdf[\"gene_id\"] = tdf[\"transcript_id\"]\n",
    "    tdf[\"expected_count\"] = 0.00\n",
    "    tdf[\"FPKM\"] = 0.00\n",
    "    tdf[\"IsoPct\"] = 0.00\n",
    "    tdf[rsem_tpm_cols].to_csv(out_dir+\"/res_distrib.real.sample\"+str(i)+\".rsem_tpms\",sep=\"\\t\",index=False)\n",
    "\n",
    "    edf = nonint_gtf[nonint_gtf[\"type\"]==\"exon\"].reset_index(drop=True)\n",
    "    tdf = nonint_gtf[nonint_gtf[\"type\"]==\"transcript\"][[\"tid\",\"start\",\"end\"]].reset_index(drop=True) # intended for order\n",
    "    tdf[\"len\"] = tdf[\"end\"]-tdf[\"start\"]\n",
    "    tdf.drop([\"start\",\"end\"],axis=1,inplace=True)\n",
    "    edf[\"elen\"] = edf[\"end\"]-edf[\"start\"]\n",
    "    edf = edf[[\"tid\",\"elen\"]]\n",
    "    edf = edf.groupby(\"tid\").agg({\"elen\":\"sum\"}).reset_index()\n",
    "    # now organize by\n",
    "    tdf = tdf.merge(edf,on=\"tid\",how=\"left\",indicator=True)\n",
    "    assert len(tdf) == len(tdf[tdf[\"_merge\"]==\"both\"]),\"missing/extra exons\"\n",
    "\n",
    "    tdf[\"cov\"] = nonint_exp[\"nonints\"]\n",
    "    tdf[\"cov\"] = tdf[\"cov\"].fillna(0)\n",
    "    assert len(tdf[tdf[\"cov\"]==0])==0,\"missing coverages\"\n",
    "    tdf[\"nr\"] = tdf[\"elen\"]*tdf[\"cov\"]\n",
    "    tdf[\"kelen\"] = tdf[\"elen\"]/1000\n",
    "    tdf[\"rpk\"] = tdf[\"nr\"]/tdf[\"kelen\"]\n",
    "    pm_sf = tdf[\"rpk\"].sum()/1000000\n",
    "    tdf[\"tpm\"] = tdf[\"rpk\"]/pm_sf\n",
    "    tdf.drop([\"_merge\",\"cov\",\"nr\",\"kelen\",\"rpk\"],axis=1,inplace=True)\n",
    "    tdf.columns = [\"transcript_id\",\"length\",\"effective_length\",\"TPM\"]\n",
    "    tdf[\"gene_id\"] = tdf[\"transcript_id\"]\n",
    "    tdf[\"expected_count\"] = 0.00\n",
    "    tdf[\"FPKM\"] = 0.00\n",
    "    tdf[\"IsoPct\"] = 0.00\n",
    "    tdf[rsem_tpm_cols].to_csv(out_dir+\"/res_distrib.nonint.sample\"+str(i)+\".rsem_tpms\",sep=\"\\t\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  1\n",
       "1  2\n",
       "2  3\n",
       "3  4"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [1,2,3,4]\n",
    "pd.DataFrame(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript_id</th>\n",
       "      <th>gene_id</th>\n",
       "      <th>length</th>\n",
       "      <th>effective_length</th>\n",
       "      <th>expected_count</th>\n",
       "      <th>TPM</th>\n",
       "      <th>FPKM</th>\n",
       "      <th>IsoPct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL_00201782</td>\n",
       "      <td>ALL_00201782</td>\n",
       "      <td>2444</td>\n",
       "      <td>1130</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.904005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALL_00201784</td>\n",
       "      <td>ALL_00201784</td>\n",
       "      <td>1418</td>\n",
       "      <td>1418</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.261712</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ALL_00201785</td>\n",
       "      <td>ALL_00201785</td>\n",
       "      <td>1373</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.184178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ALL_00201786</td>\n",
       "      <td>ALL_00201786</td>\n",
       "      <td>2003</td>\n",
       "      <td>1224</td>\n",
       "      <td>0.0</td>\n",
       "      <td>236.832432</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ALL_00201787</td>\n",
       "      <td>ALL_00201787</td>\n",
       "      <td>5573</td>\n",
       "      <td>1987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.765111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19231</th>\n",
       "      <td>ALL_00339167</td>\n",
       "      <td>ALL_00339167</td>\n",
       "      <td>500130</td>\n",
       "      <td>1157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.169205</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19232</th>\n",
       "      <td>ALL_00330244</td>\n",
       "      <td>ALL_00330244</td>\n",
       "      <td>103254</td>\n",
       "      <td>2308</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.927062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19233</th>\n",
       "      <td>ALL_00330257</td>\n",
       "      <td>ALL_00330257</td>\n",
       "      <td>41718</td>\n",
       "      <td>902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>447.635194</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19234</th>\n",
       "      <td>ALL_00330260</td>\n",
       "      <td>ALL_00330260</td>\n",
       "      <td>4766</td>\n",
       "      <td>2887</td>\n",
       "      <td>0.0</td>\n",
       "      <td>174.279707</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19235</th>\n",
       "      <td>ALL_00064023</td>\n",
       "      <td>ALL_00064023</td>\n",
       "      <td>4180</td>\n",
       "      <td>1378</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.939004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19236 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      transcript_id       gene_id  length  effective_length  expected_count  \\\n",
       "0      ALL_00201782  ALL_00201782    2444              1130             0.0   \n",
       "1      ALL_00201784  ALL_00201784    1418              1418             0.0   \n",
       "2      ALL_00201785  ALL_00201785    1373              1024             0.0   \n",
       "3      ALL_00201786  ALL_00201786    2003              1224             0.0   \n",
       "4      ALL_00201787  ALL_00201787    5573              1987             0.0   \n",
       "...             ...           ...     ...               ...             ...   \n",
       "19231  ALL_00339167  ALL_00339167  500130              1157             0.0   \n",
       "19232  ALL_00330244  ALL_00330244  103254              2308             0.0   \n",
       "19233  ALL_00330257  ALL_00330257   41718               902             0.0   \n",
       "19234  ALL_00330260  ALL_00330260    4766              2887             0.0   \n",
       "19235  ALL_00064023  ALL_00064023    4180              1378             0.0   \n",
       "\n",
       "              TPM  FPKM  IsoPct  \n",
       "0        7.904005   0.0     0.0  \n",
       "1        2.261712   0.0     0.0  \n",
       "2       55.184178   0.0     0.0  \n",
       "3      236.832432   0.0     0.0  \n",
       "4       10.765111   0.0     0.0  \n",
       "...           ...   ...     ...  \n",
       "19231   54.169205   0.0     0.0  \n",
       "19232    0.927062   0.0     0.0  \n",
       "19233  447.635194   0.0     0.0  \n",
       "19234  174.279707   0.0     0.0  \n",
       "19235    4.939004   0.0     0.0  \n",
       "\n",
       "[19236 rows x 8 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_real = pd.read_csv(out_dir+\"/res_distrib.real.sample0.rsem_tpms.tmp\",sep=\"\\t\")\n",
    "exp_real"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
