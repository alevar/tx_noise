{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import random\n",
    "from scipy import stats\n",
    "import glob\n",
    "import math\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "import seaborn as sns\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "plt.rcParams['figure.figsize'] = (20.0, 10.0)\n",
    "plt.rcParams['font.family'] = \"serif\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declarations\n",
    "out_dir = \"/ccb/salz8-1/avaraby/tx_noise/analysis_21042020/\"\n",
    "\n",
    "num_tissues = 3\n",
    "num_samples = 10\n",
    "\n",
    "num_processes = 4\n",
    "\n",
    "gff3cols=[\"seqid\",\"source\",\"type\",\"start\",\"end\",\"score\",\"strand\",\"phase\",\"attributes\"]\n",
    "\n",
    "# bcbioRnaseq - both salmon and kallisto can be aggregated to the gene-level\n",
    "# https://github.com/bcbio/bcbio-nextgen/issues/2077"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gene_res(ts):\n",
    "    tn = ts[0]\n",
    "    sn = ts[1]\n",
    "    print(\"\\n=================\\nTissue #\"+str(tn)+\" - Sample #\"+str(sn)+\"\\n=================\\n\")\n",
    "    tx_nr = dict()\n",
    "\n",
    "    # first we need to get the number of simulated reads per each gene for each group\n",
    "    with open(out_dir+\"all.t\"+str(tn)+\"_s\"+str(sn)+\"/sample_01.shuffled.fasta\",\"r\") as inFP:\n",
    "        for line in inFP.readlines():\n",
    "            if line[0]==\">\":\n",
    "                tid = line.split(\"/\")[1].split(\";\")[0]\n",
    "                tx_nr[tid] = tx_nr.setdefault(tid,0)+1\n",
    "                \n",
    "    print(\"loaded fasta\")\n",
    "\n",
    "    # now build respective dataframes\n",
    "    counts = pd.DataFrame(tx_nr,index=[0]).T.reset_index()\n",
    "    counts.columns = [\"tid\",\"nr\"]\n",
    "\n",
    "    # now need to load the base GTF to link intronic and splicing reads to the respective genes\n",
    "    real = pd.read_csv(out_dir+\"real.t\"+str(tn)+\"_s\"+str(sn)+\".gtf\",sep=\"\\t\",names=gff3cols)\n",
    "    real = real[real[\"type\"]==\"transcript\"].reset_index(drop=True)\n",
    "    real[\"gid\"] = real[\"attributes\"].str.split(\"gene_id \\\"\",expand=True,n=1)[1].str.split(\"\\\"\",expand=True,n=1)[0]\n",
    "    real[\"tid\"] = real[\"attributes\"].str.split(\"transcript_id \\\"\",expand=True,n=1)[1].str.split(\"\\\"\",expand=True,n=1)[0]\n",
    "    real = real[[\"gid\",\"tid\"]]\n",
    "    real[\"type\"] = \"real\"\n",
    "\n",
    "    splicing = pd.read_csv(out_dir+\"splicing.t\"+str(tn)+\"_s\"+str(sn)+\".gtf\",sep=\"\\t\",names=gff3cols)\n",
    "    splicing = splicing[splicing[\"type\"]==\"transcript\"].reset_index(drop=True)\n",
    "    splicing[\"gid\"] = splicing[\"attributes\"].str.split(\"gene_id \\\"\",expand=True,n=1)[1].str.split(\"\\\"\",expand=True,n=1)[0]\n",
    "    splicing[\"tid\"] = splicing[\"attributes\"].str.split(\"transcript_id \\\"\",expand=True,n=1)[1].str.split(\"\\\"\",expand=True,n=1)[0]\n",
    "    splicing = splicing[[\"gid\",\"tid\"]]\n",
    "    splicing[\"type\"] = \"noise\"\n",
    "\n",
    "    intronic = pd.read_csv(out_dir+\"intronic.t\"+str(tn)+\"_s\"+str(sn)+\".gtf\",sep=\"\\t\",names=gff3cols)\n",
    "    intronic = intronic[intronic[\"type\"]==\"transcript\"].reset_index(drop=True)\n",
    "    intronic[\"gid\"] = intronic[\"attributes\"].str.split(\"gene_id \\\"\",expand=True,n=1)[1].str.split(\"\\\"\",expand=True,n=1)[0]\n",
    "    intronic[\"tid\"] = intronic[\"attributes\"].str.split(\"transcript_id \\\"\",expand=True,n=1)[1].str.split(\"\\\"\",expand=True,n=1)[0]\n",
    "    intronic = intronic[[\"gid\",\"tid\"]]\n",
    "    intronic[\"type\"] = \"noise\"\n",
    "\n",
    "    intergenic = pd.read_csv(out_dir+\"intergenic.t\"+str(tn)+\"_s\"+str(sn)+\".gtf\",sep=\"\\t\",names=gff3cols)\n",
    "    intergenic = intergenic[intergenic[\"type\"]==\"transcript\"].reset_index(drop=True)\n",
    "    intergenic[\"gid\"] = intergenic[\"attributes\"].str.split(\"gene_id \\\"\",expand=True,n=1)[1].str.split(\"\\\"\",expand=True,n=1)[0]\n",
    "    intergenic[\"tid\"] = intergenic[\"attributes\"].str.split(\"transcript_id \\\"\",expand=True,n=1)[1].str.split(\"\\\"\",expand=True,n=1)[0]\n",
    "    intergenic = intergenic[[\"gid\",\"tid\"]]\n",
    "    intergenic[\"type\"] = \"noise\"\n",
    "    \n",
    "    print(\"loaded gtf\")\n",
    "\n",
    "    # aggregate the \n",
    "    all_df = pd.concat([real,splicing,intronic,intergenic],axis=0)\n",
    "\n",
    "    # now need to add counts to the transcripts\n",
    "    all_df = all_df.merge(counts,how=\"outer\",indicator=True,on=\"tid\")\n",
    "    assert len(all_df[all_df[\"_merge\"]==\"right_only\"])==0,\"unidentified transcripts found\"\n",
    "    all_df.dropna(axis=0,inplace=True)\n",
    "    all_df.drop([\"_merge\",\"tid\"],axis=1,inplace=True)\n",
    "    \n",
    "    print(\"added counts\")\n",
    "\n",
    "    # now need to aggregate the results by gene and type\n",
    "    all_df = all_df.groupby(by=[\"gid\",\"type\"]).sum().reset_index()\n",
    "\n",
    "    real = all_df[all_df[\"type\"]==\"real\"].reset_index(drop=True)\n",
    "    real.drop(\"type\",axis=1,inplace=True)\n",
    "    real.columns = [\"gid\",\"nr_real\"]\n",
    "    noise = all_df[all_df[\"type\"]==\"noise\"].reset_index(drop=True)\n",
    "    noise.drop(\"type\",axis=1,inplace=True)\n",
    "    noise.columns = [\"gid\",\"nr_noise\"]\n",
    "    all_df = real.merge(noise,how=\"outer\",on=\"gid\")\n",
    "\n",
    "    all_df.replace(np.nan,0,inplace=True)\n",
    "    all_df[\"nr_total\"] = all_df[\"nr_real\"]+all_df[\"nr_noise\"]\n",
    "    all_df[\"frac_real\"] = all_df[\"nr_real\"]/all_df[\"nr_total\"]\n",
    "    \n",
    "    print(\"aggregated\")\n",
    "\n",
    "    # now need to load the results\n",
    "    res_real = pd.read_csv(out_dir+\"real.t\"+str(tn)+\"_s\"+str(sn)+\".res\")\n",
    "    # again need to aggregate the res_realults for gene level\n",
    "    res_real[\"gid\"] = \"CHS.\"+res_real[\"tid\"].str.split(\".\",expand=True)[1]\n",
    "    res_real = res_real[[\"gid\",\"sim_nreads\",\"strg_nreads\",\"slmn_nreads\",\"klst_nreads\"]]\n",
    "    res_real = res_real.groupby(by=\"gid\").sum().reset_index()\n",
    "    res_real.columns = [\"gid\",\"sim_nreads\",\"strg_nreads_real\",\"slmn_nreads_real\",\"klst_nreads_real\"]\n",
    "\n",
    "    res_all = pd.read_csv(out_dir+\"all.t\"+str(tn)+\"_s\"+str(sn)+\".res\")\n",
    "    # again need to aggregate the res_allults for gene level\n",
    "    res_all[\"gid\"] = \"CHS.\"+res_all[\"tid\"].str.split(\".\",expand=True)[1]\n",
    "    res_all = res_all[[\"gid\",\"sim_nreads\",\"strg_nreads\",\"slmn_nreads\",\"klst_nreads\"]]\n",
    "    res_all = res_all.groupby(by=\"gid\").sum().reset_index()\n",
    "    res_all.columns = [\"gid\",\"sim_nreads_all\",\"strg_nreads_all\",\"slmn_nreads_all\",\"klst_nreads_all\"]\n",
    "\n",
    "\n",
    "    # now we can combine this data with the fractions\n",
    "    res = res_real.merge(res_all,how=\"outer\",on=\"gid\",indicator=True)\n",
    "    assert len(res[res[\"_merge\"]==\"both\"])==len(res),\"unidentified genes\"\n",
    "    assert len(res[res[\"sim_nreads\"]==res[\"sim_nreads_all\"]])==len(res),\"non-matching number of reads\"\n",
    "    res.drop([\"sim_nreads_all\",\"_merge\"],axis=1,inplace=True)\n",
    "    \n",
    "    print(\"combined\")\n",
    "\n",
    "    # now we can separate false positives and false negatives\n",
    "    fp_strg_real = res[(res[\"sim_nreads\"]==0)&(res[\"strg_nreads_real\"]>0)].reset_index(drop=True)\n",
    "    fp_slmn_real = res[(res[\"sim_nreads\"]==0)&(res[\"slmn_nreads_real\"]>0)].reset_index(drop=True)\n",
    "    fp_klst_real = res[(res[\"sim_nreads\"]==0)&(res[\"klst_nreads_real\"]>0)].reset_index(drop=True)\n",
    "\n",
    "    fp_strg_all = res[(res[\"sim_nreads\"]==0)&(res[\"strg_nreads_all\"]>0)].reset_index(drop=True)\n",
    "    fp_slmn_all = res[(res[\"sim_nreads\"]==0)&(res[\"slmn_nreads_all\"]>0)].reset_index(drop=True)\n",
    "    fp_klst_all = res[(res[\"sim_nreads\"]==0)&(res[\"klst_nreads_all\"]>0)].reset_index(drop=True)\n",
    "\n",
    "    fn_strg_real = res[(res[\"sim_nreads\"]>0)&(res[\"strg_nreads_real\"]==0)].reset_index(drop=True)\n",
    "    fn_slmn_real = res[(res[\"sim_nreads\"]>0)&(res[\"slmn_nreads_real\"]==0)].reset_index(drop=True)\n",
    "    fn_klst_real = res[(res[\"sim_nreads\"]>0)&(res[\"klst_nreads_real\"]==0)].reset_index(drop=True)\n",
    "\n",
    "    fn_strg_all = res[(res[\"sim_nreads\"]>0)&(res[\"strg_nreads_all\"]==0)].reset_index(drop=True)\n",
    "    fn_slmn_all = res[(res[\"sim_nreads\"]>0)&(res[\"slmn_nreads_all\"]==0)].reset_index(drop=True)\n",
    "    fn_klst_all = res[(res[\"sim_nreads\"]>0)&(res[\"klst_nreads_all\"]==0)].reset_index(drop=True)\n",
    "\n",
    "    fp_strg_real.to_csv(out_dir+\"fp_strg_real.t\"+str(tn)+\"_s\"+str(sn)+\".csv\",index=False)\n",
    "    fp_slmn_real.to_csv(out_dir+\"fp_slmn_real.t\"+str(tn)+\"_s\"+str(sn)+\".csv\",index=False)\n",
    "    fp_klst_real.to_csv(out_dir+\"fp_klst_real.t\"+str(tn)+\"_s\"+str(sn)+\".csv\",index=False)\n",
    "\n",
    "    fp_strg_all.to_csv(out_dir+\"fp_strg_all.t\"+str(tn)+\"_s\"+str(sn)+\".csv\",index=False)\n",
    "    fp_slmn_all.to_csv(out_dir+\"fp_slmn_all.t\"+str(tn)+\"_s\"+str(sn)+\".csv\",index=False)\n",
    "    fp_klst_all.to_csv(out_dir+\"fp_klst_all.t\"+str(tn)+\"_s\"+str(sn)+\".csv\",index=False)\n",
    "\n",
    "    fn_strg_real.to_csv(out_dir+\"fn_strg_real.t\"+str(tn)+\"_s\"+str(sn)+\".csv\",index=False)\n",
    "    fn_slmn_real.to_csv(out_dir+\"fn_slmn_real.t\"+str(tn)+\"_s\"+str(sn)+\".csv\",index=False)\n",
    "    fn_klst_real.to_csv(out_dir+\"fn_klst_real.t\"+str(tn)+\"_s\"+str(sn)+\".csv\",index=False)\n",
    "\n",
    "    fn_strg_all.to_csv(out_dir+\"fn_strg_all.t\"+str(tn)+\"_s\"+str(sn)+\".csv\",index=False)\n",
    "    fn_slmn_all.to_csv(out_dir+\"fn_slmn_all.t\"+str(tn)+\"_s\"+str(sn)+\".csv\",index=False)\n",
    "    fn_klst_all.to_csv(out_dir+\"fn_klst_all.t\"+str(tn)+\"_s\"+str(sn)+\".csv\",index=False)\n",
    "    \n",
    "    print(\"saved\")\n",
    "\n",
    "    # now we need to add fractions\n",
    "    strg_real = res[(res[\"sim_nreads\"]>0)&(res[\"strg_nreads_real\"]>0)][[\"gid\",\"sim_nreads\",\"strg_nreads_real\"]].reset_index(drop=True)\n",
    "    slmn_real = res[(res[\"sim_nreads\"]>0)&(res[\"slmn_nreads_real\"]>0)][[\"gid\",\"sim_nreads\",\"slmn_nreads_real\"]].reset_index(drop=True)\n",
    "    klst_real = res[(res[\"sim_nreads\"]>0)&(res[\"klst_nreads_real\"]>0)][[\"gid\",\"sim_nreads\",\"klst_nreads_real\"]].reset_index(drop=True)\n",
    "\n",
    "    strg_all = res[(res[\"sim_nreads\"]>0)&(res[\"strg_nreads_all\"]>0)][[\"gid\",\"sim_nreads\",\"strg_nreads_all\"]].reset_index(drop=True)\n",
    "    slmn_all = res[(res[\"sim_nreads\"]>0)&(res[\"slmn_nreads_all\"]>0)][[\"gid\",\"sim_nreads\",\"slmn_nreads_all\"]].reset_index(drop=True)\n",
    "    klst_all = res[(res[\"sim_nreads\"]>0)&(res[\"klst_nreads_all\"]>0)][[\"gid\",\"sim_nreads\",\"klst_nreads_all\"]].reset_index(drop=True)\n",
    "\n",
    "    strg_real = strg_real.merge(all_df[[\"gid\",\"frac_real\"]],how=\"outer\",on=\"gid\",indicator=True)\n",
    "    assert len(strg_real[strg_real[\"_merge\"]==\"left_only\"])==0,\"unidentified transcripts\"\n",
    "    strg_real.drop(\"_merge\",axis=1,inplace=True)\n",
    "\n",
    "    slmn_real = slmn_real.merge(all_df[[\"gid\",\"frac_real\"]],how=\"outer\",on=\"gid\",indicator=True)\n",
    "    assert len(slmn_real[slmn_real[\"_merge\"]==\"left_only\"])==0,\"unidentified transcripts\"\n",
    "    slmn_real.drop(\"_merge\",axis=1,inplace=True)\n",
    "\n",
    "    klst_real = klst_real.merge(all_df[[\"gid\",\"frac_real\"]],how=\"outer\",on=\"gid\",indicator=True)\n",
    "    assert len(klst_real[klst_real[\"_merge\"]==\"left_only\"])==0,\"unidentified transcripts\"\n",
    "    klst_real.drop(\"_merge\",axis=1,inplace=True)\n",
    "\n",
    "    strg_all = strg_all.merge(all_df[[\"gid\",\"frac_real\"]],how=\"outer\",on=\"gid\",indicator=True)\n",
    "    assert len(strg_all[strg_all[\"_merge\"]==\"left_only\"])==0,\"unidentified transcripts\"\n",
    "    strg_all.drop(\"_merge\",axis=1,inplace=True)\n",
    "\n",
    "    slmn_all = slmn_all.merge(all_df[[\"gid\",\"frac_real\"]],how=\"outer\",on=\"gid\",indicator=True)\n",
    "    assert len(slmn_all[slmn_all[\"_merge\"]==\"left_only\"])==0,\"unidentified transcripts\"\n",
    "    slmn_all.drop(\"_merge\",axis=1,inplace=True)\n",
    "\n",
    "    klst_all = klst_all.merge(all_df[[\"gid\",\"frac_real\"]],how=\"outer\",on=\"gid\",indicator=True)\n",
    "    assert len(klst_all[klst_all[\"_merge\"]==\"left_only\"])==0,\"unidentified transcripts\"\n",
    "    klst_all.drop(\"_merge\",axis=1,inplace=True)\n",
    "    \n",
    "    print(\"got fractions\")\n",
    "\n",
    "    # compute fold change\n",
    "    strg_real[\"fold\"] = (strg_real[\"strg_nreads_real\"]-strg_real[\"sim_nreads\"])/strg_real[\"sim_nreads\"]\n",
    "    strg_real.drop([\"sim_nreads\",\"strg_nreads_real\"],axis=1,inplace=True)\n",
    "    strg_real.dropna(axis=0,inplace=True)\n",
    "    strg_real = strg_real.round({'frac_real':1})\n",
    "    slmn_real[\"fold\"] = (slmn_real[\"slmn_nreads_real\"]-slmn_real[\"sim_nreads\"])/slmn_real[\"sim_nreads\"]\n",
    "    slmn_real.drop([\"sim_nreads\",\"slmn_nreads_real\"],axis=1,inplace=True)\n",
    "    slmn_real.dropna(axis=0,inplace=True)\n",
    "    slmn_real = slmn_real.round({'frac_real':1})\n",
    "    klst_real[\"fold\"] = (klst_real[\"klst_nreads_real\"]-klst_real[\"sim_nreads\"])/klst_real[\"sim_nreads\"]\n",
    "    klst_real.drop([\"sim_nreads\",\"klst_nreads_real\"],axis=1,inplace=True)\n",
    "    klst_real.dropna(axis=0,inplace=True)\n",
    "    klst_real = klst_real.round({'frac_real':1})\n",
    "\n",
    "    strg_all[\"fold\"] = (strg_all[\"strg_nreads_all\"]-strg_all[\"sim_nreads\"])/strg_all[\"sim_nreads\"]\n",
    "    strg_all.drop([\"sim_nreads\",\"strg_nreads_all\"],axis=1,inplace=True)\n",
    "    strg_all.dropna(axis=0,inplace=True)\n",
    "    strg_all = strg_all.round({'frac_real':1})\n",
    "    slmn_all[\"fold\"] = (slmn_all[\"slmn_nreads_all\"]-slmn_all[\"sim_nreads\"])/slmn_all[\"sim_nreads\"]\n",
    "    slmn_all.drop([\"sim_nreads\",\"slmn_nreads_all\"],axis=1,inplace=True)\n",
    "    slmn_all.dropna(axis=0,inplace=True)\n",
    "    slmn_all = slmn_all.round({'frac_real':1})\n",
    "    klst_all[\"fold\"] = (klst_all[\"klst_nreads_all\"]-klst_all[\"sim_nreads\"])/klst_all[\"sim_nreads\"]\n",
    "    klst_all.drop([\"sim_nreads\",\"klst_nreads_all\"],axis=1,inplace=True)\n",
    "    klst_all.dropna(axis=0,inplace=True)\n",
    "    klst_all = klst_all.round({'frac_real':1})\n",
    "    \n",
    "    print(\"computed fold and saving\")\n",
    "\n",
    "    strg_real.to_csv(out_dir+\"strg_real.t\"+str(tn)+\"_s\"+str(sn)+\".csv\",index=False)\n",
    "    strg_all.to_csv(out_dir+\"strg_all.t\"+str(tn)+\"_s\"+str(sn)+\".csv\",index=False)\n",
    "\n",
    "    slmn_real.to_csv(out_dir+\"slmn_real.t\"+str(tn)+\"_s\"+str(sn)+\".csv\",index=False)\n",
    "    slmn_all.to_csv(out_dir+\"slmn_all.t\"+str(tn)+\"_s\"+str(sn)+\".csv\",index=False)\n",
    "\n",
    "    klst_real.to_csv(out_dir+\"klst_real.t\"+str(tn)+\"_s\"+str(sn)+\".csv\",index=False)\n",
    "    klst_all.to_csv(out_dir+\"klst_all.t\"+str(tn)+\"_s\"+str(sn)+\".csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "samples = []\n",
    "for tn in range(num_tissues):\n",
    "    for sn in range(num_samples):\n",
    "        samples.append((tn,sn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================\n",
      "Tissue #0 - Sample #2\n",
      "=================\n",
      "\n",
      "=================\n",
      "Tissue #0 - Sample #4\n",
      "=================\n",
      "\n",
      "=================\n",
      "Tissue #0 - Sample #0\n",
      "=================\n",
      "\n",
      "=================\n",
      "Tissue #0 - Sample #6\n",
      "=================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "loaded fasta\n",
      "loaded gtf\n",
      "added counts\n",
      "aggregated\n",
      "combined\n",
      "saved\n",
      "got fractions\n",
      "computed fold and saving\n",
      "\n",
      "=================\n",
      "Tissue #0 - Sample #3\n",
      "=================\n",
      "\n",
      "loaded fasta\n",
      "loaded gtf\n",
      "added counts\n",
      "aggregated\n",
      "loaded fasta\n",
      "combined\n",
      "saved\n",
      "got fractions\n",
      "computed fold and saving\n",
      "\n",
      "=================\n",
      "Tissue #0 - Sample #5\n",
      "=================\n",
      "\n",
      "loaded gtf\n",
      "added counts\n",
      "aggregated\n",
      "combined\n",
      "saved\n",
      "got fractions\n",
      "computed fold and saving\n",
      "\n",
      "=================\n",
      "Tissue #0 - Sample #7\n",
      "=================\n",
      "\n",
      "loaded fasta\n",
      "loaded gtf\n",
      "added counts\n",
      "aggregated\n",
      "combined\n",
      "saved\n",
      "got fractions\n",
      "computed fold and saving\n",
      "\n",
      "=================\n",
      "Tissue #0 - Sample #1\n",
      "=================\n",
      "\n",
      "loaded fasta\n",
      "loaded gtf\n",
      "added counts\n",
      "aggregated\n",
      "combined\n",
      "saved\n",
      "got fractions\n",
      "computed fold and saving\n",
      "\n",
      "=================\n",
      "Tissue #0 - Sample #8\n",
      "=================\n",
      "\n",
      "loaded fasta\n",
      "loaded gtf\n",
      "added counts\n",
      "aggregated\n",
      "combined\n",
      "saved\n",
      "got fractions\n",
      "computed fold and saving\n",
      "\n",
      "=================\n",
      "Tissue #1 - Sample #0\n",
      "=================\n",
      "\n",
      "loaded fasta\n",
      "loaded gtf\n",
      "added counts\n",
      "aggregated\n",
      "combined\n",
      "saved\n",
      "got fractions\n",
      "computed fold and saving\n",
      "\n",
      "=================\n",
      "Tissue #1 - Sample #2\n",
      "=================\n",
      "\n",
      "loaded fasta\n",
      "loaded fasta\n",
      "loaded gtf\n",
      "added counts\n",
      "aggregated\n",
      "loaded gtf\n",
      "added counts\n",
      "aggregated\n",
      "combined\n",
      "combined\n",
      "saved\n",
      "got fractions\n",
      "computed fold and saving\n",
      "saved\n",
      "got fractions\n",
      "computed fold and saving\n",
      "\n",
      "=================\n",
      "Tissue #0 - Sample #9\n",
      "=================\n",
      "\n",
      "\n",
      "=================\n",
      "Tissue #1 - Sample #1\n",
      "=================\n",
      "\n",
      "loaded fasta\n",
      "loaded gtf\n",
      "added counts\n",
      "aggregated\n",
      "combined\n",
      "saved\n",
      "got fractions\n",
      "computed fold and saving\n",
      "\n",
      "=================\n",
      "Tissue #1 - Sample #4\n",
      "=================\n",
      "\n",
      "loaded fasta\n",
      "loaded gtf\n",
      "added counts\n",
      "aggregated\n",
      "combined\n",
      "saved\n",
      "got fractions\n",
      "computed fold and saving\n",
      "\n",
      "=================\n",
      "Tissue #1 - Sample #3\n",
      "=================\n",
      "\n",
      "loaded fasta\n",
      "loaded gtf\n",
      "added counts\n",
      "aggregated\n",
      "combined\n",
      "saved\n",
      "got fractions\n",
      "computed fold and saving\n",
      "\n",
      "=================\n",
      "Tissue #1 - Sample #6\n",
      "=================\n",
      "\n",
      "loaded fasta\n",
      "loaded gtf\n",
      "added counts\n",
      "aggregated\n",
      "combined\n",
      "saved\n",
      "got fractions\n",
      "loaded fasta\n",
      "computed fold and saving\n",
      "\n",
      "=================\n",
      "Tissue #1 - Sample #5\n",
      "=================\n",
      "\n",
      "loaded gtf\n",
      "added counts\n",
      "aggregated\n",
      "combined\n",
      "saved\n",
      "got fractions\n",
      "computed fold and saving\n",
      "\n",
      "=================\n",
      "Tissue #1 - Sample #8\n",
      "=================\n",
      "\n",
      "loaded fasta\n",
      "loaded gtf\n",
      "added counts\n",
      "aggregated\n",
      "combined\n",
      "saved\n",
      "got fractions\n",
      "computed fold and saving\n",
      "\n",
      "=================\n",
      "Tissue #2 - Sample #0\n",
      "=================\n",
      "\n",
      "loaded fasta\n",
      "loaded gtf\n",
      "added counts\n",
      "aggregated\n",
      "combined\n",
      "saved\n",
      "got fractions\n",
      "computed fold and saving\n",
      "\n",
      "=================\n",
      "Tissue #1 - Sample #7\n",
      "=================\n",
      "\n",
      "loaded fasta\n",
      "loaded gtf\n",
      "added counts\n",
      "aggregated\n",
      "combined\n",
      "saved\n",
      "got fractions\n",
      "computed fold and saving\n",
      "\n",
      "=================\n",
      "Tissue #2 - Sample #2\n",
      "=================\n",
      "\n",
      "loaded fasta\n",
      "loaded gtf\n",
      "added counts\n",
      "aggregated\n",
      "combined\n",
      "saved\n",
      "got fractions\n",
      "computed fold and saving\n",
      "\n",
      "=================\n",
      "Tissue #1 - Sample #9\n",
      "=================\n",
      "\n",
      "loaded fasta\n",
      "loaded gtf\n",
      "added counts\n",
      "aggregated\n",
      "combined\n",
      "saved\n",
      "got fractions\n",
      "computed fold and saving\n",
      "\n",
      "=================\n",
      "Tissue #2 - Sample #1\n",
      "=================\n",
      "\n",
      "loaded fasta\n",
      "loaded gtf\n",
      "added counts\n",
      "aggregated\n",
      "combined\n",
      "saved\n",
      "got fractions\n",
      "computed fold and saving\n",
      "\n",
      "=================\n",
      "Tissue #2 - Sample #3\n",
      "=================\n",
      "\n",
      "loaded fasta\n",
      "loaded gtf\n",
      "added counts\n",
      "aggregated\n",
      "combined\n",
      "saved\n",
      "got fractions\n",
      "computed fold and saving\n",
      "\n",
      "=================\n",
      "Tissue #2 - Sample #4\n",
      "=================\n",
      "\n",
      "loaded fasta\n",
      "loaded gtf\n",
      "added counts\n",
      "aggregated\n",
      "combined\n",
      "saved\n",
      "got fractions\n",
      "computed fold and saving\n",
      "\n",
      "=================\n",
      "Tissue #2 - Sample #6\n",
      "=================\n",
      "\n",
      "loaded fasta\n",
      "loaded gtf\n",
      "added counts\n",
      "aggregated\n",
      "combined\n",
      "saved\n",
      "got fractions\n",
      "computed fold and saving\n",
      "\n",
      "=================\n",
      "Tissue #2 - Sample #8\n",
      "=================\n",
      "\n",
      "loaded fasta\n",
      "loaded gtf\n",
      "added counts\n",
      "aggregated\n",
      "combined\n",
      "saved\n",
      "got fractions\n",
      "computed fold and saving\n",
      "loaded fasta\n",
      "loaded gtf\n",
      "added counts\n",
      "aggregated\n",
      "combined\n",
      "saved\n",
      "got fractions\n",
      "computed fold and saving\n",
      "\n",
      "=================\n",
      "Tissue #2 - Sample #5\n",
      "=================\n",
      "\n",
      "loaded fasta\n",
      "loaded gtf\n",
      "added counts\n",
      "aggregated\n",
      "loaded fasta\n",
      "combined\n",
      "saved\n",
      "got fractions\n",
      "computed fold and saving\n",
      "loaded gtf\n",
      "added counts\n",
      "aggregated\n",
      "\n",
      "=================\n",
      "Tissue #2 - Sample #9\n",
      "=================\n",
      "\n",
      "combined\n",
      "saved\n",
      "got fractions\n",
      "computed fold and saving\n",
      "\n",
      "=================\n",
      "Tissue #2 - Sample #7\n",
      "=================\n",
      "\n",
      "loaded fasta\n",
      "loaded gtf\n",
      "added counts\n",
      "aggregated\n",
      "combined\n",
      "saved\n",
      "got fractions\n",
      "computed fold and saving\n",
      "loaded fasta\n",
      "loaded gtf\n",
      "added counts\n",
      "aggregated\n",
      "combined\n",
      "saved\n",
      "got fractions\n",
      "computed fold and saving\n",
      "loaded fasta\n",
      "loaded gtf\n",
      "added counts\n",
      "aggregated\n",
      "combined\n",
      "saved\n",
      "got fractions\n",
      "computed fold and saving\n",
      "Pool: [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "pool = multiprocessing.Pool(processes=num_processes)\n",
    "pool_outputs = pool.map(get_gene_res, samples)\n",
    "pool.close()\n",
    "pool.join()\n",
    "print('Pool:', pool_outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
