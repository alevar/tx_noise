{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import random\n",
    "from scipy import stats\n",
    "import glob\n",
    "import math\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "import seaborn as sns\n",
    "plt.rcParams['figure.figsize'] = (20.0, 10.0)\n",
    "plt.rcParams['font.family'] = \"serif\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declarations\n",
    "base_dir_data = \"/ccb/salz8-1/avaraby/tx_noise/data/\"\n",
    "base_dir_out = \"/ccb/salz8-1/avaraby/tx_noise/full_analysis_t3_s10_26022020/GTEx_aggs/\"\n",
    "out_dir = \"/ccb/salz8-1/avaraby/tx_noise/full_analysis_t3_s10_26022020/sim_samples/\"\n",
    "\n",
    "num_tissues = 3\n",
    "num_samples = 10\n",
    "\n",
    "readlen = 101\n",
    "\n",
    "gff3cols=[\"seqid\",\"source\",\"type\",\"start\",\"end\",\"score\",\"strand\",\"phase\",\"attributes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we need to implement a method for converting TPM information into\n",
    "# coverage information for polyester\n",
    "# we can do this by following the REM implementation of the TPM to number of reads conversion\n",
    "\n",
    "# first load the distribution of the number of reads per sample\n",
    "readlen_stats = pd.read_csv(\"/ccb/salz8-1/avaraby/tx_noise/readlen.stats\",usecols=[\"readlen\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================\n",
      "Tissue #0\n",
      "=================\n",
      "\n",
      "++++++\n",
      ">Sample #0\n",
      "++++++\n",
      "\n",
      "number of reads in sample is: 101738610.05267522\n",
      "++++++\n",
      ">Sample #1\n",
      "++++++\n",
      "\n",
      "number of reads in sample is: 98569602.98587418\n",
      "++++++\n",
      ">Sample #2\n",
      "++++++\n",
      "\n",
      "number of reads in sample is: 137750021.2740215\n",
      "++++++\n",
      ">Sample #3\n",
      "++++++\n",
      "\n",
      "number of reads in sample is: 68749304.48132825\n",
      "++++++\n",
      ">Sample #4\n",
      "++++++\n",
      "\n",
      "number of reads in sample is: 115624138.52164288\n",
      "++++++\n",
      ">Sample #5\n",
      "++++++\n",
      "\n",
      "number of reads in sample is: 90354803.03818865\n",
      "++++++\n",
      ">Sample #6\n",
      "++++++\n",
      "\n",
      "number of reads in sample is: 117620128.12841591\n",
      "++++++\n",
      ">Sample #7\n",
      "++++++\n",
      "\n",
      "number of reads in sample is: 36547972.27686375\n",
      "++++++\n",
      ">Sample #8\n",
      "++++++\n",
      "\n",
      "number of reads in sample is: 180693854.62017268\n",
      "++++++\n",
      ">Sample #9\n",
      "++++++\n",
      "\n",
      "number of reads in sample is: 64676948.234055206\n",
      "\n",
      "=================\n",
      "Tissue #1\n",
      "=================\n",
      "\n",
      "++++++\n",
      ">Sample #0\n",
      "++++++\n",
      "\n",
      "number of reads in sample is: 98500070.72957925\n",
      "++++++\n",
      ">Sample #1\n",
      "++++++\n",
      "\n",
      "number of reads in sample is: 76826927.76816139\n",
      "++++++\n",
      ">Sample #2\n",
      "++++++\n",
      "\n",
      "number of reads in sample is: 139347922.52277154\n",
      "++++++\n",
      ">Sample #3\n",
      "++++++\n",
      "\n",
      "number of reads in sample is: 81861070.64810745\n",
      "++++++\n",
      ">Sample #4\n",
      "++++++\n",
      "\n",
      "number of reads in sample is: 90619670.29573703\n",
      "++++++\n",
      ">Sample #5\n",
      "++++++\n",
      "\n",
      "number of reads in sample is: 133488508.20531341\n",
      "++++++\n",
      ">Sample #6\n",
      "++++++\n",
      "\n",
      "number of reads in sample is: 110334594.39378902\n",
      "++++++\n",
      ">Sample #7\n",
      "++++++\n",
      "\n",
      "number of reads in sample is: 155866640.10261908\n",
      "++++++\n",
      ">Sample #8\n",
      "++++++\n",
      "\n",
      "number of reads in sample is: 154330116.81897467\n",
      "++++++\n",
      ">Sample #9\n",
      "++++++\n",
      "\n",
      "number of reads in sample is: 142347428.33929414\n",
      "\n",
      "=================\n",
      "Tissue #2\n",
      "=================\n",
      "\n",
      "++++++\n",
      ">Sample #0\n",
      "++++++\n",
      "\n",
      "number of reads in sample is: 127502318.30287747\n",
      "++++++\n",
      ">Sample #1\n",
      "++++++\n",
      "\n",
      "number of reads in sample is: 116694939.88466793\n",
      "++++++\n",
      ">Sample #2\n",
      "++++++\n",
      "\n",
      "number of reads in sample is: 96404757.94790441\n",
      "++++++\n",
      ">Sample #3\n",
      "++++++\n",
      "\n",
      "number of reads in sample is: 80461552.20007041\n",
      "++++++\n",
      ">Sample #4\n",
      "++++++\n",
      "\n",
      "number of reads in sample is: 94266238.21498653\n",
      "++++++\n",
      ">Sample #5\n",
      "++++++\n",
      "\n",
      "number of reads in sample is: 117575819.55796066\n",
      "++++++\n",
      ">Sample #6\n",
      "++++++\n",
      "\n",
      "number of reads in sample is: 61100492.07736036\n",
      "++++++\n",
      ">Sample #7\n",
      "++++++\n",
      "\n",
      "number of reads in sample is: 121230434.05337301\n",
      "++++++\n",
      ">Sample #8\n",
      "++++++\n",
      "\n",
      "number of reads in sample is: 104043709.92112714\n",
      "++++++\n",
      ">Sample #9\n",
      "++++++\n",
      "\n",
      "number of reads in sample is: 141500524.50891703\n"
     ]
    }
   ],
   "source": [
    "for tissue_num in range(num_tissues):\n",
    "    print(\"\\n=================\\nTissue #\"+str(tissue_num)+\"\\n=================\\n\")\n",
    "    for sample_num in range(num_samples):\n",
    "        print(\"++++++\\n>Sample #\"+str(sample_num)+\"\\n++++++\\n\")\n",
    "        total_nreads = np.random.normal(readlen_stats[\"readlen\"].mean(),readlen_stats[\"readlen\"].std())\n",
    "        print(\"number of reads in sample is: \"+str(total_nreads))\n",
    "\n",
    "        real = pd.read_csv(out_dir+\"real.t\"+str(tissue_num)+\"_s\"+str(sample_num)+\".gtf\",sep=\"\\t\",names=gff3cols)\n",
    "        real[\"tid\"] = real[\"attributes\"].str.split(\"transcript_id \\\"\",expand=True)[1].str.split(\"\\\"\",expand=True)[0]\n",
    "        real[\"gid\"] = real[\"attributes\"].str.split(\"gene_id \\\"\",expand=True)[1].str.split(\"\\\"\",expand=True)[0]\n",
    "        realt = real[real[\"type\"]==\"transcript\"][[\"tid\",\"gid\"]].reset_index(drop=True) # intended for order\n",
    "        reale = real[real[\"type\"]==\"exon\"].reset_index(drop=True)\n",
    "        reale[\"elen\"] = reale[\"end\"]-reale[\"start\"]\n",
    "        reale = reale[[\"tid\",\"elen\"]]\n",
    "        reale = reale.groupby(\"tid\").agg({\"elen\":\"sum\"}).reset_index()\n",
    "        assert set(realt[\"tid\"])==set(reale[\"tid\"]),\"number of transcripts is not the same as number of groupped exons\"\n",
    "        reale = realt.merge(reale,how=\"left\",on=\"tid\")\n",
    "        tpms = pd.read_csv(out_dir+\"real.t\"+str(tissue_num)+\"_s\"+str(sample_num)+\".exp\",names=[\"tpm\"])\n",
    "        assert len(tpms)==len(reale),\"number of tpms different from the number of transcripts\"\n",
    "        reale[\"tpm\"] = tpms[\"tpm\"]\n",
    "\n",
    "        splicing = pd.read_csv(out_dir+\"splicing.t\"+str(tissue_num)+\"_s\"+str(sample_num)+\".gtf\",sep=\"\\t\",names=gff3cols)\n",
    "        splicing[\"tid\"] = splicing[\"attributes\"].str.split(\"transcript_id \\\"\",expand=True)[1].str.split(\"\\\"\",expand=True)[0]\n",
    "        splicing[\"gid\"] = splicing[\"attributes\"].str.split(\"gene_id \\\"\",expand=True)[1].str.split(\"\\\"\",expand=True)[0]\n",
    "        splicingt = splicing[splicing[\"type\"]==\"transcript\"][[\"tid\",\"gid\"]].reset_index(drop=True) # intended for order\n",
    "        splicinge = splicing[splicing[\"type\"]==\"exon\"].reset_index(drop=True)\n",
    "        splicinge[\"elen\"] = splicinge[\"end\"]-splicinge[\"start\"]\n",
    "        splicinge = splicinge[[\"tid\",\"elen\"]]\n",
    "        splicinge = splicinge.groupby(\"tid\").agg({\"elen\":\"sum\"}).reset_index()\n",
    "        assert set(splicingt[\"tid\"])==set(splicinge[\"tid\"]),\"number of transcripts is not the same as number of groupped exons\"\n",
    "        splicinge = splicingt.merge(splicinge,how=\"left\",on=\"tid\")\n",
    "        tpms = pd.read_csv(out_dir+\"splicing.t\"+str(tissue_num)+\"_s\"+str(sample_num)+\".exp\",names=[\"tpm\"])\n",
    "        assert len(tpms)==len(splicinge),\"number of tpms different from the number of transcripts\"\n",
    "        splicinge[\"tpm\"] = tpms[\"tpm\"]\n",
    "\n",
    "        intronic = pd.read_csv(out_dir+\"intronic.t\"+str(tissue_num)+\"_s\"+str(sample_num)+\".gtf\",sep=\"\\t\",names=gff3cols)\n",
    "        intronic[\"tid\"] = intronic[\"attributes\"].str.split(\"transcript_id \\\"\",expand=True)[1].str.split(\"\\\"\",expand=True)[0]\n",
    "        intronic[\"gid\"] = intronic[\"attributes\"].str.split(\"gene_id \\\"\",expand=True)[1].str.split(\"\\\"\",expand=True)[0]\n",
    "        intronict = intronic[intronic[\"type\"]==\"transcript\"][[\"tid\",\"gid\"]].reset_index(drop=True) # intended for order\n",
    "        intronice = intronic[intronic[\"type\"]==\"exon\"].reset_index(drop=True)\n",
    "        intronice[\"elen\"] = intronice[\"end\"]-intronice[\"start\"]\n",
    "        intronice = intronice[[\"tid\",\"elen\"]]\n",
    "        intronice = intronice.groupby(\"tid\").agg({\"elen\":\"sum\"}).reset_index()\n",
    "        assert set(intronict[\"tid\"])==set(intronice[\"tid\"]),\"number of transcripts is not the same as number of groupped exons\"\n",
    "        intronice = intronict.merge(intronice,how=\"left\",on=\"tid\")\n",
    "        tpms = pd.read_csv(out_dir+\"intronic.t\"+str(tissue_num)+\"_s\"+str(sample_num)+\".exp\",names=[\"tpm\"])\n",
    "        assert len(tpms)==len(intronice),\"number of tpms different from the number of transcripts\"\n",
    "        intronice[\"tpm\"] = tpms[\"tpm\"]\n",
    "\n",
    "        pol = pd.read_csv(out_dir+\"intergenic.t\"+str(tissue_num)+\"_s\"+str(sample_num)+\".gtf\",sep=\"\\t\",names=gff3cols)\n",
    "        pol[\"tid\"] = pol[\"attributes\"].str.split(\"transcript_id \\\"\",expand=True)[1].str.split(\"\\\"\",expand=True)[0]\n",
    "        pol[\"gid\"] = pol[\"attributes\"].str.split(\"gene_id \\\"\",expand=True)[1].str.split(\"\\\"\",expand=True)[0]\n",
    "        polt = pol[pol[\"type\"]==\"transcript\"][[\"tid\",\"gid\"]].reset_index(drop=True) # intended for order\n",
    "        pole = pol[pol[\"type\"]==\"exon\"].reset_index(drop=True)\n",
    "        pole[\"elen\"] = pole[\"end\"]-pole[\"start\"]\n",
    "        pole = pole[[\"tid\",\"elen\"]]\n",
    "        pole = pole.groupby(\"tid\").agg({\"elen\":\"sum\"}).reset_index()\n",
    "        assert set(polt[\"tid\"])==set(pole[\"tid\"]),\"number of transcripts is not the same as number of groupped exons\"\n",
    "        pole = polt.merge(pole,how=\"left\",on=\"tid\")\n",
    "\n",
    "        tpms = pd.read_csv(out_dir+\"intergenic.t\"+str(tissue_num)+\"_s\"+str(sample_num)+\".exp\",names=[\"tpm\"])\n",
    "        assert len(tpms)==len(pole),\"number of tpms different from the number of transcripts\"\n",
    "        pole[\"tpm\"] = tpms[\"tpm\"]\n",
    "        joined = pd.concat([reale[[\"tid\",\"elen\",\"tpm\"]],splicinge[[\"tid\",\"elen\",\"tpm\"]],intronice[[\"tid\",\"elen\",\"tpm\"]],pole[[\"tid\",\"elen\",\"tpm\"]]],axis=0).reset_index(drop=True)\n",
    "        joined[\"theta\"] = joined[\"elen\"]*joined[\"tpm\"]\n",
    "        denom = joined[\"theta\"].sum()\n",
    "        joined[\"cor\"] = joined[\"theta\"]/denom\n",
    "        # now that we have all these values, we \n",
    "        joined[\"cov\"] = (joined[\"cor\"]*total_nreads*readlen)/joined[\"elen\"]\n",
    "        # now we can merge the data to comply with the original ordering\n",
    "        # and proceed to write it out\n",
    "        realt.merge(joined[[\"tid\",\"cov\"]],how=\"left\",on=\"tid\")[[\"cov\"]].to_csv(out_dir+\"real.t\"+str(tissue_num)+\"_s\"+str(sample_num)+\".cov\",index=False,header=False)\n",
    "        splicingt.merge(joined[[\"tid\",\"cov\"]],how=\"left\",on=\"tid\")[[\"cov\"]].to_csv(out_dir+\"splicing.t\"+str(tissue_num)+\"_s\"+str(sample_num)+\".cov\",index=False,header=False)\n",
    "        intronict.merge(joined[[\"tid\",\"cov\"]],how=\"left\",on=\"tid\")[[\"cov\"]].to_csv(out_dir+\"intronic.t\"+str(tissue_num)+\"_s\"+str(sample_num)+\".cov\",index=False,header=False)\n",
    "        polt.merge(joined[[\"tid\",\"cov\"]],how=\"left\",on=\"tid\")[[\"cov\"]].to_csv(out_dir+\"intergenic.t\"+str(tissue_num)+\"_s\"+str(sample_num)+\".cov\",index=False,header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
